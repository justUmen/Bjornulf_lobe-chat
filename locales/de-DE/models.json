{
  "01-ai/Yi-1.5-34B-Chat-16K": {
    "description": "Yi-1.5 34B bietet mit umfangreichen Trainingsbeispielen überlegene Leistungen in der Branchenanwendung."
  },
  "01-ai/Yi-1.5-9B-Chat-16K": {
    "description": "Yi-1.5 9B unterstützt 16K Tokens und bietet effiziente, flüssige Sprachgenerierungsfähigkeiten."
  },
  "360gpt-pro": {
    "description": "360GPT Pro ist ein wichtiger Bestandteil der 360 AI-Modellreihe und erfüllt mit seiner effizienten Textverarbeitungsfähigkeit vielfältige Anwendungen der natürlichen Sprache, unterstützt das Verständnis langer Texte und Mehrfachdialoge."
  },
  "360gpt-turbo": {
    "description": "360GPT Turbo bietet leistungsstarke Berechnungs- und Dialogfähigkeiten, mit hervorragendem semantischen Verständnis und Generierungseffizienz, und ist die ideale intelligente Assistentenlösung für Unternehmen und Entwickler."
  },
  "360gpt-turbo-responsibility-8k": {
    "description": "360GPT Turbo Responsibility 8K betont semantische Sicherheit und verantwortungsbewusste Ausrichtung, speziell für Anwendungen mit hohen Anforderungen an die Inhaltssicherheit konzipiert, um die Genauigkeit und Robustheit der Benutzererfahrung zu gewährleisten."
  },
  "360gpt2-pro": {
    "description": "360GPT2 Pro ist ein fortschrittliches Modell zur Verarbeitung natürlicher Sprache, das von der 360 Company entwickelt wurde und über außergewöhnliche Textgenerierungs- und Verständnisfähigkeiten verfügt, insbesondere im Bereich der Generierung und Kreativität, und in der Lage ist, komplexe Sprachumwandlungs- und Rollendarstellungsaufgaben zu bewältigen."
  },
  "4.0Ultra": {
    "description": "Spark4.0 Ultra ist die leistungsstärkste Version der Spark-Großmodellreihe, die die Online-Suchverbindung aktualisiert und die Fähigkeit zur Textverständnis und -zusammenfassung verbessert. Es ist eine umfassende Lösung zur Steigerung der Büroproduktivität und zur genauen Reaktion auf Anforderungen und ein führendes intelligentes Produkt in der Branche."
  },
  "Baichuan2-Turbo": {
    "description": "Verwendet Suchverbesserungstechnologie, um eine umfassende Verknüpfung zwischen großen Modellen und Fachwissen sowie Wissen aus dem gesamten Internet zu ermöglichen. Unterstützt das Hochladen von Dokumenten wie PDF, Word und die Eingabe von URLs, um Informationen zeitnah und umfassend zu erhalten, mit genauen und professionellen Ergebnissen."
  },
  "Baichuan3-Turbo": {
    "description": "Für häufige Unternehmensszenarien optimiert, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verhältnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-Fähigkeit um 40 % verbessert. Die Gesamtleistung übertrifft die von GPT-3.5."
  },
  "Baichuan3-Turbo-128k": {
    "description": "Verfügt über ein 128K Ultra-Langkontextfenster, optimiert für häufige Unternehmensszenarien, mit erheblichen Leistungssteigerungen und einem hohen Preis-Leistungs-Verhältnis. Im Vergleich zum Baichuan2-Modell wurde die Inhaltserstellung um 20 %, die Wissensabfrage um 17 % und die Rollenspiel-Fähigkeit um 40 % verbessert. Die Gesamtleistung übertrifft die von GPT-3.5."
  },
  "Baichuan4": {
    "description": "Das Modell hat die höchste Fähigkeit im Inland und übertrifft ausländische Mainstream-Modelle in Aufgaben wie Wissensdatenbanken, langen Texten und kreativer Generierung. Es verfügt auch über branchenführende multimodale Fähigkeiten und zeigt in mehreren autoritativen Bewertungsbenchmarks hervorragende Leistungen."
  },
  "Gryphe/MythoMax-L2-13b": {
    "description": "MythoMax-L2 (13B) ist ein innovatives Modell, das sich für Anwendungen in mehreren Bereichen und komplexe Aufgaben eignet."
  },
  "Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Hermes 2 Mixtral 8x7B DPO ist eine hochflexible Multi-Modell-Kombination, die darauf abzielt, außergewöhnliche kreative Erlebnisse zu bieten."
  },
  "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO": {
    "description": "Nous Hermes 2 - Mixtral 8x7B-DPO (46.7B) ist ein hochpräzises Anweisungsmodell, das für komplexe Berechnungen geeignet ist."
  },
  "NousResearch/Nous-Hermes-2-Yi-34B": {
    "description": "Nous Hermes-2 Yi (34B) bietet optimierte Sprachausgaben und vielfältige Anwendungsmöglichkeiten."
  },
  "Phi-3-5-mini-instruct": {
    "description": "Aktualisierung des Phi-3-mini-Modells."
  },
  "Phi-3-medium-128k-instruct": {
    "description": "Das gleiche Phi-3-medium-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-medium-4k-instruct": {
    "description": "Ein Modell mit 14 Milliarden Parametern, das eine bessere Qualität als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."
  },
  "Phi-3-mini-128k-instruct": {
    "description": "Das gleiche Phi-3-mini-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-mini-4k-instruct": {
    "description": "Das kleinste Mitglied der Phi-3-Familie. Optimiert für Qualität und geringe Latenz."
  },
  "Phi-3-small-128k-instruct": {
    "description": "Das gleiche Phi-3-small-Modell, jedoch mit einer größeren Kontextgröße für RAG oder Few-Shot-Prompting."
  },
  "Phi-3-small-8k-instruct": {
    "description": "Ein Modell mit 7 Milliarden Parametern, das eine bessere Qualität als Phi-3-mini bietet und sich auf qualitativ hochwertige, reasoning-dense Daten konzentriert."
  },
  "Pro-128k": {
    "description": "Spark Pro-128K ist mit einer extrem großen Kontextverarbeitungsfähigkeit ausgestattet, die bis zu 128K Kontextinformationen verarbeiten kann, besonders geeignet für lange Texte, die eine umfassende Analyse und langfristige logische Verknüpfung erfordern, und bietet in komplexen Textkommunikationen flüssige und konsistente Logik sowie vielfältige Zitationsunterstützung."
  },
  "Qwen/Qwen1.5-110B-Chat": {
    "description": "Als Testversion von Qwen2 bietet Qwen1.5 präzisere Dialogfunktionen durch den Einsatz großer Datenmengen."
  },
  "Qwen/Qwen1.5-72B-Chat": {
    "description": "Qwen 1.5 Chat (72B) bietet schnelle Antworten und natürliche Dialogfähigkeiten, die sich für mehrsprachige Umgebungen eignen."
  },
  "Qwen/Qwen2-72B-Instruct": {
    "description": "Qwen2 ist ein fortschrittliches allgemeines Sprachmodell, das eine Vielzahl von Anweisungsarten unterstützt."
  },
  "Qwen/Qwen2.5-14B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-32B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-72B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen mit verbesserter Verständnis- und Generierungsfähigkeit."
  },
  "Qwen/Qwen2.5-7B-Instruct": {
    "description": "Qwen2.5 ist eine brandneue Serie von großen Sprachmodellen, die darauf abzielt, die Verarbeitung von Anweisungsaufgaben zu optimieren."
  },
  "Qwen/Qwen2.5-Coder-7B-Instruct": {
    "description": "Qwen2.5-Coder konzentriert sich auf die Programmierung."
  },
  "Qwen/Qwen2.5-Math-72B-Instruct": {
    "description": "Qwen2.5-Math konzentriert sich auf die Problemlösung im Bereich Mathematik und bietet professionelle Lösungen für schwierige Aufgaben."
  },
  "THUDM/glm-4-9b-chat": {
    "description": "GLM-4 9B ist die Open-Source-Version, die ein optimiertes Dialogerlebnis für Konversationsanwendungen bietet."
  },
  "abab5.5-chat": {
    "description": "Für produktivitätsorientierte Szenarien konzipiert, unterstützt es die Verarbeitung komplexer Aufgaben und die effiziente Textgenerierung, geeignet für professionelle Anwendungen."
  },
  "abab5.5s-chat": {
    "description": "Speziell für chinesische Charakterdialoge konzipiert, bietet es hochwertige chinesische Dialoggenerierung und ist für verschiedene Anwendungsszenarien geeignet."
  },
  "abab6.5g-chat": {
    "description": "Speziell für mehrsprachige Charakterdialoge konzipiert, unterstützt die hochwertige Dialoggenerierung in Englisch und anderen Sprachen."
  },
  "abab6.5s-chat": {
    "description": "Geeignet für eine Vielzahl von Aufgaben der natürlichen Sprachverarbeitung, einschließlich Textgenerierung und Dialogsystemen."
  },
  "abab6.5t-chat": {
    "description": "Für chinesische Charakterdialoge optimiert, bietet es flüssige und den chinesischen Ausdrucksgewohnheiten entsprechende Dialoggenerierung."
  },
  "accounts/fireworks/models/firefunction-v1": {
    "description": "Das Open-Source-Funktionsaufrufmodell von Fireworks bietet hervorragende Anweisungsdurchführungsfähigkeiten und anpassbare Funktionen."
  },
  "accounts/fireworks/models/firefunction-v2": {
    "description": "Das neueste Firefunction-v2 von Fireworks ist ein leistungsstarkes Funktionsaufrufmodell, das auf Llama-3 basiert und durch zahlreiche Optimierungen besonders für Funktionsaufrufe, Dialoge und Anweisungsverfolgung geeignet ist."
  },
  "accounts/fireworks/models/firellava-13b": {
    "description": "fireworks-ai/FireLLaVA-13b ist ein visuelles Sprachmodell, das sowohl Bild- als auch Texteingaben verarbeiten kann und für multimodale Aufgaben geeignet ist, nachdem es mit hochwertigen Daten trainiert wurde."
  },
  "accounts/fireworks/models/gemma2-9b-it": {
    "description": "Das Gemma 2 9B Instruct-Modell basiert auf früheren Google-Technologien und eignet sich für eine Vielzahl von Textgenerierungsaufgaben wie Fragen beantworten, Zusammenfassen und Schlussfolgern."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct": {
    "description": "Das Llama 3 70B Instruct-Modell ist speziell für mehrsprachige Dialoge und natürliche Sprachverständnis optimiert und übertrifft die meisten Wettbewerbsmodelle."
  },
  "accounts/fireworks/models/llama-v3-70b-instruct-hf": {
    "description": "Das Llama 3 70B Instruct-Modell (HF-Version) entspricht den offiziellen Ergebnissen und eignet sich für hochwertige Anweisungsverfolgungsaufgaben."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct": {
    "description": "Das Llama 3 8B Instruct-Modell ist für Dialoge und mehrsprachige Aufgaben optimiert und bietet hervorragende und effiziente Leistungen."
  },
  "accounts/fireworks/models/llama-v3-8b-instruct-hf": {
    "description": "Das Llama 3 8B Instruct-Modell (HF-Version) stimmt mit den offiziellen Ergebnissen überein und bietet hohe Konsistenz und plattformübergreifende Kompatibilität."
  },
  "accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "description": "Das Llama 3.1 405B Instruct-Modell verfügt über eine extrem große Anzahl von Parametern und eignet sich für komplexe Aufgaben und Anweisungsverfolgung in hochbelasteten Szenarien."
  },
  "accounts/fireworks/models/llama-v3p1-70b-instruct": {
    "description": "Das Llama 3.1 70B Instruct-Modell bietet hervorragende natürliche Sprachverständnis- und Generierungsfähigkeiten und ist die ideale Wahl für Dialog- und Analyseaufgaben."
  },
  "accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "description": "Das Llama 3.1 8B Instruct-Modell ist speziell für mehrsprachige Dialoge optimiert und kann die meisten Open-Source- und Closed-Source-Modelle in gängigen Branchenbenchmarks übertreffen."
  },
  "accounts/fireworks/models/mixtral-8x22b-instruct": {
    "description": "Das Mixtral MoE 8x22B Instruct-Modell unterstützt durch seine große Anzahl an Parametern und Multi-Expert-Architektur die effiziente Verarbeitung komplexer Aufgaben."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct": {
    "description": "Das Mixtral MoE 8x7B Instruct-Modell bietet durch seine Multi-Expert-Architektur effiziente Anweisungsverfolgung und -ausführung."
  },
  "accounts/fireworks/models/mixtral-8x7b-instruct-hf": {
    "description": "Das Mixtral MoE 8x7B Instruct-Modell (HF-Version) bietet die gleiche Leistung wie die offizielle Implementierung und eignet sich für verschiedene effiziente Anwendungsszenarien."
  },
  "accounts/fireworks/models/mythomax-l2-13b": {
    "description": "Das MythoMax L2 13B-Modell kombiniert neuartige Kombinations-Technologien und ist besonders gut in Erzählungen und Rollenspielen."
  },
  "accounts/fireworks/models/phi-3-vision-128k-instruct": {
    "description": "Das Phi 3 Vision Instruct-Modell ist ein leichtgewichtiges multimodales Modell, das komplexe visuelle und textuelle Informationen verarbeiten kann und über starke Schlussfolgerungsfähigkeiten verfügt."
  },
  "accounts/fireworks/models/starcoder-16b": {
    "description": "Das StarCoder 15.5B-Modell unterstützt fortgeschrittene Programmieraufgaben und hat verbesserte mehrsprachige Fähigkeiten, die sich für komplexe Codegenerierung und -verständnis eignen."
  },
  "accounts/fireworks/models/starcoder-7b": {
    "description": "Das StarCoder 7B-Modell wurde für über 80 Programmiersprachen trainiert und bietet hervorragende Programmierausfüllfähigkeiten und Kontextverständnis."
  },
  "accounts/yi-01-ai/models/yi-large": {
    "description": "Das Yi-Large-Modell bietet hervorragende mehrsprachige Verarbeitungsfähigkeiten und kann für verschiedene Sprachgenerierungs- und Verständnisaufgaben eingesetzt werden."
  },
  "ai21-jamba-1.5-large": {
    "description": "Ein mehrsprachiges Modell mit 398 Milliarden Parametern (94 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."
  },
  "ai21-jamba-1.5-mini": {
    "description": "Ein mehrsprachiges Modell mit 52 Milliarden Parametern (12 Milliarden aktiv), das ein 256K langes Kontextfenster, Funktionsaufrufe, strukturierte Ausgaben und fundierte Generierung bietet."
  },
  "ai21-jamba-instruct": {
    "description": "Ein produktionsreifes Mamba-basiertes LLM-Modell, das eine erstklassige Leistung, Qualität und Kosteneffizienz erreicht."
  },
  "anthropic.claude-3-5-sonnet-20240620-v1:0": {
    "description": "Claude 3.5 Sonnet hebt den Branchenstandard an, übertrifft die Konkurrenzmodelle und Claude 3 Opus und zeigt in umfassenden Bewertungen hervorragende Leistungen, während es die Geschwindigkeit und Kosten unserer mittleren Modelle beibehält."
  },
  "anthropic.claude-3-haiku-20240307-v1:0": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic und bietet nahezu sofortige Reaktionsgeschwindigkeiten. Es kann schnell einfache Anfragen und Anforderungen beantworten. Kunden werden in der Lage sein, nahtlose AI-Erlebnisse zu schaffen, die menschliche Interaktionen nachahmen. Claude 3 Haiku kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-3-opus-20240229-v1:0": {
    "description": "Claude 3 Opus ist das leistungsstärkste AI-Modell von Anthropic mit fortschrittlicher Leistung bei hochkomplexen Aufgaben. Es kann offene Eingaben und unbekannte Szenarien verarbeiten und zeigt hervorragende Flüssigkeit und menschenähnliches Verständnis. Claude 3 Opus demonstriert die Grenzen der Möglichkeiten generativer AI. Claude 3 Opus kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-3-sonnet-20240229-v1:0": {
    "description": "Anthropic's Claude 3 Sonnet erreicht ein ideales Gleichgewicht zwischen Intelligenz und Geschwindigkeit – besonders geeignet für Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem Preis, der unter dem der Konkurrenz liegt, und wurde als zuverlässiges, langlebiges Hauptmodell für skalierbare AI-Implementierungen konzipiert. Claude 3 Sonnet kann Bilder verarbeiten und Textausgaben zurückgeben, mit einem Kontextfenster von 200K."
  },
  "anthropic.claude-instant-v1": {
    "description": "Ein schnelles, kostengünstiges und dennoch sehr leistungsfähiges Modell, das eine Reihe von Aufgaben bewältigen kann, darunter alltägliche Gespräche, Textanalysen, Zusammenfassungen und Dokumentenfragen."
  },
  "anthropic.claude-v2": {
    "description": "Anthropic zeigt in einer Vielzahl von Aufgaben, von komplexen Dialogen und kreativer Inhaltserstellung bis hin zu detaillierten Anweisungen, ein hohes Maß an Fähigkeiten."
  },
  "anthropic.claude-v2:1": {
    "description": "Die aktualisierte Version von Claude 2 bietet ein doppelt so großes Kontextfenster sowie Verbesserungen in der Zuverlässigkeit, der Halluzinationsrate und der evidenzbasierten Genauigkeit in langen Dokumenten und RAG-Kontexten."
  },
  "anthropic/claude-3-haiku": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic, das darauf ausgelegt ist, nahezu sofortige Antworten zu liefern. Es bietet schnelle und präzise zielgerichtete Leistungen."
  },
  "anthropic/claude-3-opus": {
    "description": "Claude 3 Opus ist das leistungsstärkste Modell von Anthropic zur Bearbeitung hochkomplexer Aufgaben. Es zeichnet sich durch hervorragende Leistung, Intelligenz, Flüssigkeit und Verständnis aus."
  },
  "anthropic/claude-3.5-sonnet": {
    "description": "Claude 3.5 Sonnet bietet Fähigkeiten, die über Opus hinausgehen, und eine schnellere Geschwindigkeit als Sonnet, während es den gleichen Preis wie Sonnet beibehält. Sonnet ist besonders gut in Programmierung, Datenwissenschaft, visueller Verarbeitung und Agentenaufgaben."
  },
  "aya": {
    "description": "Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterstützt und die Anwendung in einer Vielzahl von Sprachen erleichtert."
  },
  "aya:35b": {
    "description": "Aya 23 ist ein mehrsprachiges Modell von Cohere, das 23 Sprachen unterstützt und die Anwendung in einer Vielzahl von Sprachen erleichtert."
  },
  "charglm-3": {
    "description": "CharGLM-3 ist für Rollenspiele und emotionale Begleitung konzipiert und unterstützt extrem lange Mehrfachgedächtnisse und personalisierte Dialoge, mit breiter Anwendung."
  },
  "chatgpt-4o-latest": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "claude-2.0": {
    "description": "Claude 2 bietet Unternehmen Fortschritte in kritischen Fähigkeiten, einschließlich branchenführenden 200K Token Kontext, erheblich reduzierter Häufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."
  },
  "claude-2.1": {
    "description": "Claude 2 bietet Unternehmen Fortschritte in kritischen Fähigkeiten, einschließlich branchenführenden 200K Token Kontext, erheblich reduzierter Häufigkeit von Modellillusionen, Systemaufforderungen und einer neuen Testfunktion: Werkzeugaufrufe."
  },
  "claude-3-5-sonnet-20240620": {
    "description": "Claude 3.5 Sonnet bietet Fähigkeiten, die über Opus hinausgehen, und ist schneller als Sonnet, während es den gleichen Preis wie Sonnet beibehält. Sonnet ist besonders gut in Programmierung, Datenwissenschaft, visueller Verarbeitung und Agenturaufgaben."
  },
  "claude-3-haiku-20240307": {
    "description": "Claude 3 Haiku ist das schnellste und kompakteste Modell von Anthropic, das darauf abzielt, nahezu sofortige Antworten zu liefern. Es bietet schnelle und präzise zielgerichtete Leistungen."
  },
  "claude-3-opus-20240229": {
    "description": "Claude 3 Opus ist das leistungsstärkste Modell von Anthropic für die Verarbeitung hochkomplexer Aufgaben. Es bietet herausragende Leistungen in Bezug auf Leistung, Intelligenz, Flüssigkeit und Verständnis."
  },
  "claude-3-sonnet-20240229": {
    "description": "Claude 3 Sonnet bietet eine ideale Balance zwischen Intelligenz und Geschwindigkeit für Unternehmensarbeitslasten. Es bietet maximalen Nutzen zu einem niedrigeren Preis, ist zuverlässig und für großflächige Bereitstellungen geeignet."
  },
  "claude-instant-1.2": {
    "description": "Das Modell von Anthropic wird für latenzarme, hochdurchsatzfähige Textgenerierung verwendet und unterstützt die Generierung von Hunderten von Seiten Text."
  },
  "codegeex-4": {
    "description": "CodeGeeX-4 ist ein leistungsstarker AI-Programmierassistent, der intelligente Fragen und Codevervollständigung in verschiedenen Programmiersprachen unterstützt und die Entwicklungseffizienz steigert."
  },
  "codegemma": {
    "description": "CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell für verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterstützt."
  },
  "codegemma:2b": {
    "description": "CodeGemma ist ein leichtgewichtiges Sprachmodell, das speziell für verschiedene Programmieraufgaben entwickelt wurde und schnelle Iterationen und Integrationen unterstützt."
  },
  "codellama": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama:13b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama:34b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codellama:70b": {
    "description": "Code Llama ist ein LLM, das sich auf die Codegenerierung und -diskussion konzentriert und eine breite Unterstützung für Programmiersprachen bietet, die sich für Entwicklerumgebungen eignet."
  },
  "codeqwen": {
    "description": "CodeQwen1.5 ist ein großes Sprachmodell, das auf einer umfangreichen Code-Datenbasis trainiert wurde und speziell für die Lösung komplexer Programmieraufgaben entwickelt wurde."
  },
  "codestral": {
    "description": "Codestral ist das erste Code-Modell von Mistral AI und bietet hervorragende Unterstützung für Aufgaben der Codegenerierung."
  },
  "codestral-latest": {
    "description": "Codestral ist ein hochmodernes Generierungsmodell, das sich auf die Codegenerierung konzentriert und für Aufgaben wie das Ausfüllen von Zwischenräumen und die Codevervollständigung optimiert wurde."
  },
  "cognitivecomputations/dolphin-mixtral-8x22b": {
    "description": "Dolphin Mixtral 8x22B ist ein Modell, das für die Befolgung von Anweisungen, Dialoge und Programmierung entwickelt wurde."
  },
  "cohere-command-r": {
    "description": "Command R ist ein skalierbares generatives Modell, das auf RAG und Tool-Nutzung abzielt, um KI in Produktionsgröße für Unternehmen zu ermöglichen."
  },
  "cohere-command-r-plus": {
    "description": "Command R+ ist ein hochmodernes, RAG-optimiertes Modell, das für unternehmensgerechte Arbeitslasten konzipiert ist."
  },
  "command-r": {
    "description": "Command R ist ein LLM, das für Dialoge und Aufgaben mit langen Kontexten optimiert ist und sich besonders gut für dynamische Interaktionen und Wissensmanagement eignet."
  },
  "command-r-plus": {
    "description": "Command R+ ist ein leistungsstarkes großes Sprachmodell, das speziell für reale Unternehmensszenarien und komplexe Anwendungen entwickelt wurde."
  },
  "databricks/dbrx-instruct": {
    "description": "DBRX Instruct bietet zuverlässige Anweisungsverarbeitungsfähigkeiten und unterstützt Anwendungen in verschiedenen Branchen."
  },
  "deepseek-ai/DeepSeek-V2.5": {
    "description": "DeepSeek V2.5 vereint die hervorragenden Merkmale früherer Versionen und verbessert die allgemeinen und kodierenden Fähigkeiten."
  },
  "deepseek-ai/deepseek-llm-67b-chat": {
    "description": "DeepSeek 67B ist ein fortschrittliches Modell, das für komplexe Dialoge trainiert wurde."
  },
  "deepseek-chat": {
    "description": "Ein neues Open-Source-Modell, das allgemeine und Codefähigkeiten kombiniert. Es bewahrt nicht nur die allgemeinen Dialogfähigkeiten des ursprünglichen Chat-Modells und die leistungsstarken Codeverarbeitungsfähigkeiten des Coder-Modells, sondern stimmt auch besser mit menschlichen Präferenzen überein. Darüber hinaus hat DeepSeek-V2.5 in mehreren Bereichen wie Schreibaufgaben und Befolgung von Anweisungen erhebliche Verbesserungen erzielt."
  },
  "deepseek-coder-v2": {
    "description": "DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."
  },
  "deepseek-coder-v2:236b": {
    "description": "DeepSeek Coder V2 ist ein Open-Source-Mischexperten-Code-Modell, das in Codeaufgaben hervorragende Leistungen erbringt und mit GPT4-Turbo vergleichbar ist."
  },
  "deepseek-v2": {
    "description": "DeepSeek V2 ist ein effizientes Mixture-of-Experts-Sprachmodell, das für wirtschaftliche Verarbeitungsanforderungen geeignet ist."
  },
  "deepseek-v2:236b": {
    "description": "DeepSeek V2 236B ist das Design-Code-Modell von DeepSeek und bietet starke Fähigkeiten zur Codegenerierung."
  },
  "deepseek/deepseek-chat": {
    "description": "Ein neues Open-Source-Modell, das allgemeine und Codefähigkeiten vereint. Es behält nicht nur die allgemeinen Dialogfähigkeiten des ursprünglichen Chat-Modells und die leistungsstarken Codeverarbeitungsfähigkeiten des Coder-Modells bei, sondern stimmt auch besser mit menschlichen Vorlieben überein. Darüber hinaus hat DeepSeek-V2.5 in vielen Bereichen wie Schreibaufgaben und Befehlsbefolgung erhebliche Verbesserungen erzielt."
  },
  "emohaa": {
    "description": "Emohaa ist ein psychologisches Modell mit professionellen Beratungsfähigkeiten, das den Nutzern hilft, emotionale Probleme zu verstehen."
  },
  "gemini-1.0-pro-001": {
    "description": "Gemini 1.0 Pro 001 (Tuning) bietet stabile und anpassbare Leistung und ist die ideale Wahl für Lösungen komplexer Aufgaben."
  },
  "gemini-1.0-pro-002": {
    "description": "Gemini 1.0 Pro 002 (Tuning) bietet hervorragende multimodale Unterstützung und konzentriert sich auf die effektive Lösung komplexer Aufgaben."
  },
  "gemini-1.0-pro-latest": {
    "description": "Gemini 1.0 Pro ist Googles leistungsstarkes KI-Modell, das für die Skalierung einer Vielzahl von Aufgaben konzipiert ist."
  },
  "gemini-1.5-flash-001": {
    "description": "Gemini 1.5 Flash 001 ist ein effizientes multimodales Modell, das eine breite Anwendbarkeit unterstützt."
  },
  "gemini-1.5-flash-8b-exp-0827": {
    "description": "Gemini 1.5 Flash 8B 0827 ist für die Verarbeitung großangelegter Aufgabenszenarien konzipiert und bietet unvergleichliche Verarbeitungsgeschwindigkeit."
  },
  "gemini-1.5-flash-exp-0827": {
    "description": "Gemini 1.5 Flash 0827 bietet optimierte multimodale Verarbeitungsfähigkeiten, die für verschiedene komplexe Aufgabenszenarien geeignet sind."
  },
  "gemini-1.5-flash-latest": {
    "description": "Gemini 1.5 Flash ist Googles neuestes multimodales KI-Modell, das über schnelle Verarbeitungsfähigkeiten verfügt und Text-, Bild- und Videoeingaben unterstützt, um eine effiziente Skalierung für verschiedene Aufgaben zu ermöglichen."
  },
  "gemini-1.5-pro-001": {
    "description": "Gemini 1.5 Pro 001 ist eine skalierbare multimodale KI-Lösung, die eine breite Palette komplexer Aufgaben unterstützt."
  },
  "gemini-1.5-pro-exp-0801": {
    "description": "Gemini 1.5 Pro 0801 bietet hervorragende multimodale Verarbeitungsfähigkeiten und bringt mehr Flexibilität in die Anwendungsentwicklung."
  },
  "gemini-1.5-pro-exp-0827": {
    "description": "Gemini 1.5 Pro 0827 kombiniert die neuesten Optimierungstechniken und bietet eine effizientere multimodale Datenverarbeitung."
  },
  "gemini-1.5-pro-latest": {
    "description": "Gemini 1.5 Pro unterstützt bis zu 2 Millionen Tokens und ist die ideale Wahl für mittelgroße multimodale Modelle, die umfassende Unterstützung für komplexe Aufgaben bieten."
  },
  "gemma-7b-it": {
    "description": "Gemma 7B eignet sich für die Verarbeitung von mittelgroßen Aufgaben und bietet ein gutes Kosten-Nutzen-Verhältnis."
  },
  "gemma2": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "gemma2-9b-it": {
    "description": "Gemma 2 9B ist ein Modell, das für spezifische Aufgaben und die Integration von Werkzeugen optimiert wurde."
  },
  "gemma2:27b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "gemma2:2b": {
    "description": "Gemma 2 ist ein effizientes Modell von Google, das eine Vielzahl von Anwendungsszenarien von kleinen Anwendungen bis hin zu komplexen Datenverarbeitungen abdeckt."
  },
  "general": {
    "description": "Spark Lite ist ein leichtgewichtiges großes Sprachmodell mit extrem niedriger Latenz und hoher Verarbeitungsfähigkeit, das vollständig kostenlos und offen ist und eine Echtzeitsuchfunktion unterstützt. Seine schnelle Reaktionsfähigkeit macht es in der Inferenzanwendung und Modellanpassung auf Geräten mit geringer Rechenleistung besonders effektiv und bietet den Nutzern ein hervorragendes Kosten-Nutzen-Verhältnis und intelligente Erfahrungen, insbesondere in den Bereichen Wissensabfrage, Inhaltserstellung und Suchszenarien."
  },
  "generalv3": {
    "description": "Spark Pro ist ein hochleistungsfähiges großes Sprachmodell, das für professionelle Bereiche optimiert ist und sich auf Mathematik, Programmierung, Medizin, Bildung und andere Bereiche konzentriert, und unterstützt die Online-Suche sowie integrierte Plugins für Wetter, Datum usw. Das optimierte Modell zeigt hervorragende Leistungen und hohe Effizienz in komplexen Wissensabfragen, Sprachverständnis und hochrangiger Textgenerierung und ist die ideale Wahl für professionelle Anwendungsszenarien."
  },
  "generalv3.5": {
    "description": "Spark3.5 Max ist die umfassendste Version, die Online-Suche und zahlreiche integrierte Plugins unterstützt. Ihre umfassend optimierten Kernfähigkeiten sowie die Systemrolleneinstellungen und Funktionsaufrufmöglichkeiten ermöglichen eine außergewöhnliche Leistung in verschiedenen komplexen Anwendungsszenarien."
  },
  "glm-4": {
    "description": "GLM-4 ist die alte Flaggschiffversion, die im Januar 2024 veröffentlicht wurde und mittlerweile durch das leistungsstärkere GLM-4-0520 ersetzt wurde."
  },
  "glm-4-0520": {
    "description": "GLM-4-0520 ist die neueste Modellversion, die für hochkomplexe und vielfältige Aufgaben konzipiert wurde und hervorragende Leistungen zeigt."
  },
  "glm-4-air": {
    "description": "GLM-4-Air ist eine kosteneffiziente Version, die in der Leistung nahe am GLM-4 liegt und schnelle Geschwindigkeiten zu einem erschwinglichen Preis bietet."
  },
  "glm-4-airx": {
    "description": "GLM-4-AirX bietet eine effiziente Version von GLM-4-Air mit einer Inferenzgeschwindigkeit von bis zu 2,6-fach."
  },
  "glm-4-alltools": {
    "description": "GLM-4-AllTools ist ein multifunktionales Agentenmodell, das optimiert wurde, um komplexe Anweisungsplanung und Werkzeugaufrufe zu unterstützen, wie z. B. Web-Browsing, Code-Interpretation und Textgenerierung, geeignet für die Ausführung mehrerer Aufgaben."
  },
  "glm-4-flash": {
    "description": "GLM-4-Flash ist die ideale Wahl für die Verarbeitung einfacher Aufgaben, mit der schnellsten Geschwindigkeit und dem besten Preis."
  },
  "glm-4-long": {
    "description": "GLM-4-Long unterstützt extrem lange Texteingaben und eignet sich für Gedächtnisaufgaben und die Verarbeitung großer Dokumente."
  },
  "glm-4-plus": {
    "description": "GLM-4-Plus ist das hochintelligente Flaggschiffmodell mit starken Fähigkeiten zur Verarbeitung langer Texte und komplexer Aufgaben, mit umfassenden Leistungsverbesserungen."
  },
  "glm-4v": {
    "description": "GLM-4V bietet starke Fähigkeiten zur Bildverständnis und -schlussfolgerung und unterstützt eine Vielzahl visueller Aufgaben."
  },
  "glm-4v-plus": {
    "description": "GLM-4V-Plus hat die Fähigkeit, Videoinhalte und mehrere Bilder zu verstehen und eignet sich für multimodale Aufgaben."
  },
  "google/gemini-flash-1.5-exp": {
    "description": "Gemini 1.5 Flash 0827 bietet optimierte multimodale Verarbeitungsfähigkeiten und ist für eine Vielzahl komplexer Aufgaben geeignet."
  },
  "google/gemini-pro-1.5-exp": {
    "description": "Gemini 1.5 Pro 0827 kombiniert die neuesten Optimierungstechnologien und bietet effizientere multimodale Datenverarbeitungsfähigkeiten."
  },
  "google/gemma-2-27b-it": {
    "description": "Gemma 2 setzt das Designkonzept von Leichtbau und Effizienz fort."
  },
  "google/gemma-2-9b-it": {
    "description": "Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."
  },
  "google/gemma-2-9b-it:free": {
    "description": "Gemma 2 ist eine leichtgewichtige Open-Source-Textmodellreihe von Google."
  },
  "google/gemma-2b-it": {
    "description": "Gemma Instruct (2B) bietet grundlegende Anweisungsverarbeitungsfähigkeiten und eignet sich für leichte Anwendungen."
  },
  "gpt-3.5-turbo": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-0125": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-1106": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-3.5-turbo-instruct": {
    "description": "GPT 3.5 Turbo eignet sich für eine Vielzahl von Textgenerierungs- und Verständnisaufgaben. Derzeit verweist es auf gpt-3.5-turbo-0125."
  },
  "gpt-4": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-0125-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-0613": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-1106-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-1106-vision-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-32k": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-32k-0613": {
    "description": "GPT-4 bietet ein größeres Kontextfenster, das in der Lage ist, längere Texteingaben zu verarbeiten, und eignet sich für Szenarien, die eine umfassende Informationsintegration und Datenanalyse erfordern."
  },
  "gpt-4-turbo": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-turbo-2024-04-09": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-turbo-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4-vision-preview": {
    "description": "Das neueste GPT-4 Turbo-Modell verfügt über visuelle Funktionen. Jetzt können visuelle Anfragen im JSON-Format und durch Funktionsaufrufe gestellt werden. GPT-4 Turbo ist eine verbesserte Version, die kosteneffiziente Unterstützung für multimodale Aufgaben bietet. Es findet ein Gleichgewicht zwischen Genauigkeit und Effizienz und eignet sich für Anwendungen, die Echtzeitanpassungen erfordern."
  },
  "gpt-4o": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-2024-05-13": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "gpt-4o-mini": {
    "description": "GPT-4o mini ist das neueste Modell von OpenAI, das nach GPT-4 Omni veröffentlicht wurde und sowohl Text- als auch Bildinput unterstützt. Als ihr fortschrittlichstes kleines Modell ist es viel günstiger als andere neueste Modelle und kostet über 60 % weniger als GPT-3.5 Turbo. Es behält die fortschrittliche Intelligenz bei und bietet gleichzeitig ein hervorragendes Preis-Leistungs-Verhältnis. GPT-4o mini erzielte 82 % im MMLU-Test und rangiert derzeit in den Chat-Präferenzen über GPT-4."
  },
  "gryphe/mythomax-l2-13b": {
    "description": "MythoMax l2 13B ist ein Sprachmodell, das Kreativität und Intelligenz kombiniert und mehrere führende Modelle integriert."
  },
  "internlm/internlm2_5-20b-chat": {
    "description": "Das innovative Open-Source-Modell InternLM2.5 hat durch eine große Anzahl von Parametern die Dialogintelligenz erhöht."
  },
  "internlm/internlm2_5-7b-chat": {
    "description": "InternLM2.5 bietet intelligente Dialoglösungen in mehreren Szenarien."
  },
  "jamba-1.5-large": {},
  "jamba-1.5-mini": {},
  "llama-3.1-70b-instruct": {
    "description": "Das Llama 3.1 70B Instruct-Modell hat 70B Parameter und bietet herausragende Leistungen bei der Generierung großer Texte und Anweisungsaufgaben."
  },
  "llama-3.1-70b-versatile": {
    "description": "Llama 3.1 70B bietet leistungsstarke KI-Schlussfolgerungsfähigkeiten, die für komplexe Anwendungen geeignet sind und eine hohe Rechenverarbeitung bei gleichzeitiger Effizienz und Genauigkeit unterstützen."
  },
  "llama-3.1-8b-instant": {
    "description": "Llama 3.1 8B ist ein leistungsstarkes Modell, das schnelle Textgenerierungsfähigkeiten bietet und sich hervorragend für Anwendungen eignet, die große Effizienz und Kosteneffektivität erfordern."
  },
  "llama-3.1-8b-instruct": {
    "description": "Das Llama 3.1 8B Instruct-Modell hat 8B Parameter und unterstützt die effiziente Ausführung von bildbasierten Anweisungsaufgaben und bietet hochwertige Textgenerierungsfähigkeiten."
  },
  "llama-3.1-sonar-huge-128k-online": {
    "description": "Das Llama 3.1 Sonar Huge Online-Modell hat 405B Parameter und unterstützt eine Kontextlänge von etwa 127.000 Markierungen, es wurde für komplexe Online-Chat-Anwendungen entwickelt."
  },
  "llama-3.1-sonar-large-128k-chat": {
    "description": "Das Llama 3.1 Sonar Large Chat-Modell hat 70B Parameter und unterstützt eine Kontextlänge von etwa 127.000 Markierungen, es eignet sich für komplexe Offline-Chat-Aufgaben."
  },
  "llama-3.1-sonar-large-128k-online": {
    "description": "Das Llama 3.1 Sonar Large Online-Modell hat 70B Parameter und unterstützt eine Kontextlänge von etwa 127.000 Markierungen, es eignet sich für hochvolumige und vielfältige Chat-Aufgaben."
  },
  "llama-3.1-sonar-small-128k-chat": {
    "description": "Das Llama 3.1 Sonar Small Chat-Modell hat 8B Parameter und wurde speziell für Offline-Chat entwickelt, es unterstützt eine Kontextlänge von etwa 127.000 Markierungen."
  },
  "llama-3.1-sonar-small-128k-online": {
    "description": "Das Llama 3.1 Sonar Small Online-Modell hat 8B Parameter und unterstützt eine Kontextlänge von etwa 127.000 Markierungen, es wurde speziell für Online-Chat entwickelt und kann verschiedene Textinteraktionen effizient verarbeiten."
  },
  "llama3-70b-8192": {
    "description": "Meta Llama 3 70B bietet unvergleichliche Fähigkeiten zur Verarbeitung von Komplexität und ist maßgeschneidert für Projekte mit hohen Anforderungen."
  },
  "llama3-8b-8192": {
    "description": "Meta Llama 3 8B bietet hervorragende Schlussfolgerungsfähigkeiten und eignet sich für eine Vielzahl von Anwendungsanforderungen."
  },
  "llama3-groq-70b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 70B Tool Use bietet leistungsstarke Werkzeugaufruf-Fähigkeiten und unterstützt die effiziente Verarbeitung komplexer Aufgaben."
  },
  "llama3-groq-8b-8192-tool-use-preview": {
    "description": "Llama 3 Groq 8B Tool Use ist ein Modell, das für die effiziente Nutzung von Werkzeugen optimiert ist und schnelle parallele Berechnungen unterstützt."
  },
  "llama3.1": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llama3.1:405b": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llama3.1:70b": {
    "description": "Llama 3.1 ist ein führendes Modell von Meta, das bis zu 405B Parameter unterstützt und in den Bereichen komplexe Dialoge, mehrsprachige Übersetzungen und Datenanalysen eingesetzt werden kann."
  },
  "llava": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "llava-v1.5-7b-4096-preview": {
    "description": "LLaVA 1.5 7B bietet integrierte visuelle Verarbeitungsfähigkeiten, um komplexe Ausgaben aus visuellen Informationen zu generieren."
  },
  "llava:13b": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "llava:34b": {
    "description": "LLaVA ist ein multimodales Modell, das visuelle Encoder und Vicuna kombiniert und für starke visuelle und sprachliche Verständnisse sorgt."
  },
  "mathstral": {
    "description": "MathΣtral ist für wissenschaftliche Forschung und mathematische Schlussfolgerungen konzipiert und bietet effektive Rechenfähigkeiten und Ergebnisinterpretationen."
  },
  "meta-llama-3-70b-instruct": {
    "description": "Ein leistungsstarkes Modell mit 70 Milliarden Parametern, das in den Bereichen Schlussfolgerungen, Programmierung und breiten Sprachanwendungen herausragt."
  },
  "meta-llama-3-8b-instruct": {
    "description": "Ein vielseitiges Modell mit 8 Milliarden Parametern, das für Dialog- und Textgenerierungsaufgaben optimiert ist."
  },
  "meta-llama-3.1-405b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama-3.1-70b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama-3.1-8b-instruct": {
    "description": "Die Llama 3.1-Modelle, die auf Anweisungen optimiert sind, sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele der verfügbaren Open-Source- und geschlossenen Chat-Modelle in gängigen Branchenbenchmarks."
  },
  "meta-llama/Llama-2-13b-chat-hf": {
    "description": "LLaMA-2 Chat (13B) bietet hervorragende Sprachverarbeitungsfähigkeiten und ein ausgezeichnetes Interaktionserlebnis."
  },
  "meta-llama/Llama-3-70b-chat-hf": {
    "description": "LLaMA-3 Chat (70B) ist ein leistungsstarkes Chat-Modell, das komplexe Dialoganforderungen unterstützt."
  },
  "meta-llama/Llama-3-8b-chat-hf": {
    "description": "LLaMA-3 Chat (8B) bietet mehrsprachige Unterstützung und deckt ein breites Spektrum an Fachwissen ab."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Lite": {
    "description": "Llama 3 70B Instruct Lite ist für Umgebungen geeignet, die hohe Leistung und niedrige Latenz erfordern."
  },
  "meta-llama/Meta-Llama-3-70B-Instruct-Turbo": {
    "description": "Llama 3 70B Instruct Turbo bietet hervorragende Sprachverständnis- und Generierungsfähigkeiten und eignet sich für die anspruchsvollsten Rechenaufgaben."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Lite": {
    "description": "Llama 3 8B Instruct Lite ist für ressourcenbeschränkte Umgebungen geeignet und bietet eine hervorragende Balance zwischen Leistung und Effizienz."
  },
  "meta-llama/Meta-Llama-3-8B-Instruct-Turbo": {
    "description": "Llama 3 8B Instruct Turbo ist ein leistungsstarkes großes Sprachmodell, das eine breite Palette von Anwendungsszenarien unterstützt."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct": {
    "description": "LLaMA 3.1 405B ist ein leistungsstarkes Modell für Vortraining und Anweisungsanpassung."
  },
  "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
    "description": "Das 405B Llama 3.1 Turbo-Modell bietet eine enorme Kapazität zur Unterstützung von Kontexten für die Verarbeitung großer Datenmengen und zeigt herausragende Leistungen in groß angelegten KI-Anwendungen."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct": {
    "description": "LLaMA 3.1 70B bietet effiziente Dialogunterstützung in mehreren Sprachen."
  },
  "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
    "description": "Das Llama 3.1 70B-Modell wurde feinabgestimmt und eignet sich für hochbelastete Anwendungen, die auf FP8 quantisiert wurden, um eine effizientere Rechenleistung und Genauigkeit zu bieten und in komplexen Szenarien hervorragende Leistungen zu gewährleisten."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct": {
    "description": "LLaMA 3.1 bietet Unterstützung für mehrere Sprachen und ist eines der führenden Generierungsmodelle der Branche."
  },
  "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
    "description": "Das Llama 3.1 8B-Modell verwendet FP8-Quantisierung und unterstützt bis zu 131.072 Kontextmarkierungen, es ist eines der besten Open-Source-Modelle, das sich für komplexe Aufgaben eignet und in vielen Branchenbenchmarks übertrifft."
  },
  "meta-llama/llama-3-70b-instruct": {
    "description": "Llama 3 70B Instruct ist optimiert für qualitativ hochwertige Dialogszenarien und zeigt hervorragende Leistungen in verschiedenen menschlichen Bewertungen."
  },
  "meta-llama/llama-3-8b-instruct": {
    "description": "Llama 3 8B Instruct optimiert qualitativ hochwertige Dialogszenarien und bietet bessere Leistungen als viele geschlossene Modelle."
  },
  "meta-llama/llama-3.1-405b-instruct": {
    "description": "Llama 3.1 405B Instruct ist die neueste Version von Meta, optimiert zur Generierung qualitativ hochwertiger Dialoge und übertrifft viele führende geschlossene Modelle."
  },
  "meta-llama/llama-3.1-70b-instruct": {
    "description": "Llama 3.1 70B Instruct ist speziell für qualitativ hochwertige Dialoge konzipiert und zeigt herausragende Leistungen in menschlichen Bewertungen, besonders geeignet für hochinteraktive Szenarien."
  },
  "meta-llama/llama-3.1-8b-instruct": {
    "description": "Llama 3.1 8B Instruct ist die neueste Version von Meta, optimiert für qualitativ hochwertige Dialogszenarien und übertrifft viele führende geschlossene Modelle."
  },
  "meta-llama/llama-3.1-8b-instruct:free": {
    "description": "LLaMA 3.1 bietet Unterstützung für mehrere Sprachen und gehört zu den führenden generativen Modellen der Branche."
  },
  "meta.llama3-1-405b-instruct-v1:0": {
    "description": "Meta Llama 3.1 405B Instruct ist das größte und leistungsstärkste Modell innerhalb des Llama 3.1 Instruct Modells. Es handelt sich um ein hochentwickeltes Modell für dialogbasierte Schlussfolgerungen und die Generierung synthetischer Daten, das auch als Grundlage für die professionelle kontinuierliche Vorab- und Feinabstimmung in bestimmten Bereichen verwendet werden kann. Die mehrsprachigen großen Sprachmodelle (LLMs) von Llama 3.1 sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, die in den Größen 8B, 70B und 405B (Text-Eingabe/Ausgabe) verfügbar sind. Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind speziell für mehrsprachige Dialoganwendungen optimiert und haben in gängigen Branchenbenchmarks viele verfügbare Open-Source-Chat-Modelle übertroffen. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für verschiedene Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Das Llama 3.1 Modell unterstützt auch die Nutzung seiner Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das auf einer optimierten Transformer-Architektur basiert. Die angepasste Version verwendet überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-1-70b-instruct-v1:0": {
    "description": "Die aktualisierte Version von Meta Llama 3.1 70B Instruct umfasst eine erweiterte Kontextlänge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsfähigkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen großen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschließlich Größen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele verfügbare Open-Source-Chat-Modelle in gängigen Branchenbenchmarks. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für eine Vielzahl von Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Llama 3.1-Modelle unterstützen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-1-8b-instruct-v1:0": {
    "description": "Die aktualisierte Version von Meta Llama 3.1 8B Instruct umfasst eine erweiterte Kontextlänge von 128K, Mehrsprachigkeit und verbesserte Schlussfolgerungsfähigkeiten. Die von Llama 3.1 bereitgestellten mehrsprachigen großen Sprachmodelle (LLMs) sind eine Gruppe von vortrainierten, anweisungsoptimierten Generierungsmodellen, einschließlich Größen von 8B, 70B und 405B (Textinput/-output). Die anweisungsoptimierten Textmodelle (8B, 70B, 405B) sind für mehrsprachige Dialoganwendungen optimiert und übertreffen viele verfügbare Open-Source-Chat-Modelle in gängigen Branchenbenchmarks. Llama 3.1 ist für kommerzielle und Forschungszwecke in mehreren Sprachen konzipiert. Die anweisungsoptimierten Textmodelle eignen sich für assistentengleiche Chats, während die vortrainierten Modelle für eine Vielzahl von Aufgaben der natürlichen Sprachgenerierung angepasst werden können. Llama 3.1-Modelle unterstützen auch die Nutzung ihrer Ausgaben zur Verbesserung anderer Modelle, einschließlich der Generierung synthetischer Daten und der Verfeinerung. Llama 3.1 ist ein autoregressives Sprachmodell, das mit einer optimierten Transformer-Architektur entwickelt wurde. Die angepassten Versionen verwenden überwachte Feinabstimmung (SFT) und verstärkendes Lernen mit menschlichem Feedback (RLHF), um den menschlichen Präferenzen für Hilfsbereitschaft und Sicherheit zu entsprechen."
  },
  "meta.llama3-70b-instruct-v1:0": {
    "description": "Meta Llama 3 ist ein offenes großes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen für generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet für die Erstellung von Inhalten, Dialog-KI, Sprachverständnis, Forschung und Unternehmensanwendungen."
  },
  "meta.llama3-8b-instruct-v1:0": {
    "description": "Meta Llama 3 ist ein offenes großes Sprachmodell (LLM), das sich an Entwickler, Forscher und Unternehmen richtet und ihnen hilft, ihre Ideen für generative KI zu entwickeln, zu experimentieren und verantwortungsbewusst zu skalieren. Als Teil eines globalen Innovationssystems ist es besonders geeignet für Umgebungen mit begrenzter Rechenleistung und Ressourcen, für Edge-Geräte und schnellere Trainingszeiten."
  },
  "microsoft/wizardlm 2-7b": {
    "description": "WizardLM 2 7B ist das neueste schnelle und leichte Modell von Microsoft AI, dessen Leistung fast zehnmal so hoch ist wie die bestehender führender Open-Source-Modelle."
  },
  "microsoft/wizardlm-2-8x22b": {
    "description": "WizardLM-2 8x22B ist das fortschrittlichste Wizard-Modell von Microsoft AI und zeigt äußerst wettbewerbsfähige Leistungen."
  },
  "minicpm-v": {
    "description": "MiniCPM-V ist das neue multimodale Großmodell von OpenBMB, das über hervorragende OCR-Erkennungs- und multimodale Verständnisfähigkeiten verfügt und eine Vielzahl von Anwendungsszenarien unterstützt."
  },
  "mistral": {
    "description": "Mistral ist ein 7B-Modell von Mistral AI, das sich für vielfältige Anforderungen an die Sprachverarbeitung eignet."
  },
  "mistral-large": {
    "description": "Mixtral Large ist das Flaggschiff-Modell von Mistral, das die Fähigkeiten zur Codegenerierung, Mathematik und Schlussfolgerungen kombiniert und ein Kontextfenster von 128k unterstützt."
  },
  "mistral-large-2407": {
    "description": "Mistral Large (2407) ist ein fortschrittliches großes Sprachmodell (LLM) mit modernsten Fähigkeiten in den Bereichen Schlussfolgerungen, Wissen und Programmierung."
  },
  "mistral-large-latest": {
    "description": "Mistral Large ist das Flaggschiff-Modell, das sich gut für mehrsprachige Aufgaben, komplexe Schlussfolgerungen und Codegenerierung eignet und die ideale Wahl für hochentwickelte Anwendungen ist."
  },
  "mistral-nemo": {
    "description": "Mistral Nemo wurde in Zusammenarbeit mit Mistral AI und NVIDIA entwickelt und ist ein leistungsstarkes 12B-Modell."
  },
  "mistral-small": {
    "description": "Mistral Small kann für jede sprachbasierte Aufgabe verwendet werden, die hohe Effizienz und geringe Latenz erfordert."
  },
  "mistral-small-latest": {
    "description": "Mistral Small ist eine kosteneffiziente, schnelle und zuverlässige Option für Anwendungsfälle wie Übersetzung, Zusammenfassung und Sentimentanalyse."
  },
  "mistralai/Mistral-7B-Instruct-v0.1": {
    "description": "Mistral (7B) Instruct ist bekannt für seine hohe Leistung und eignet sich für eine Vielzahl von Sprachaufgaben."
  },
  "mistralai/Mistral-7B-Instruct-v0.2": {
    "description": "Mistral 7B ist ein nach Bedarf feinabgestimmtes Modell, das optimierte Antworten auf Aufgaben bietet."
  },
  "mistralai/Mistral-7B-Instruct-v0.3": {
    "description": "Mistral (7B) Instruct v0.3 bietet effiziente Rechenleistung und natürliche Sprachverständnisfähigkeiten und eignet sich für eine Vielzahl von Anwendungen."
  },
  "mistralai/Mixtral-8x22B-Instruct-v0.1": {
    "description": "Mixtral-8x22B Instruct (141B) ist ein super großes Sprachmodell, das extrem hohe Verarbeitungsanforderungen unterstützt."
  },
  "mistralai/Mixtral-8x7B-Instruct-v0.1": {
    "description": "Mixtral 8x7B ist ein vortrainiertes sparsames Mischmodell, das für allgemeine Textaufgaben verwendet wird."
  },
  "mistralai/mistral-7b-instruct": {
    "description": "Mistral 7B Instruct ist ein hochleistungsfähiges Branchenstandardmodell mit Geschwindigkeitsoptimierung und Unterstützung für lange Kontexte."
  },
  "mistralai/mistral-nemo": {
    "description": "Mistral Nemo ist ein 7,3B-Parameter-Modell mit Unterstützung für mehrere Sprachen und hoher Programmierleistung."
  },
  "mixtral": {
    "description": "Mixtral ist das Expertenmodell von Mistral AI, das über Open-Source-Gewichte verfügt und Unterstützung bei der Codegenerierung und Sprachverständnis bietet."
  },
  "mixtral-8x7b-32768": {
    "description": "Mixtral 8x7B bietet hochgradig fehlertolerante parallele Berechnungsfähigkeiten und eignet sich für komplexe Aufgaben."
  },
  "mixtral:8x22b": {
    "description": "Mixtral ist das Expertenmodell von Mistral AI, das über Open-Source-Gewichte verfügt und Unterstützung bei der Codegenerierung und Sprachverständnis bietet."
  },
  "moonshot-v1-128k": {
    "description": "Moonshot V1 128K ist ein Modell mit überragenden Fähigkeiten zur Verarbeitung von langen Kontexten, das für die Generierung von sehr langen Texten geeignet ist und die Anforderungen komplexer Generierungsaufgaben erfüllt. Es kann Inhalte mit bis zu 128.000 Tokens verarbeiten und eignet sich hervorragend für Anwendungen in der Forschung, Wissenschaft und der Erstellung großer Dokumente."
  },
  "moonshot-v1-32k": {
    "description": "Moonshot V1 32K bietet die Fähigkeit zur Verarbeitung von mittellangen Kontexten und kann 32.768 Tokens verarbeiten, was es besonders geeignet für die Generierung verschiedener langer Dokumente und komplexer Dialoge macht, die in den Bereichen Inhaltserstellung, Berichtsgenerierung und Dialogsysteme eingesetzt werden."
  },
  "moonshot-v1-8k": {
    "description": "Moonshot V1 8K ist für die Generierung von Kurztextaufgaben konzipiert und bietet eine effiziente Verarbeitungsleistung, die 8.192 Tokens verarbeiten kann. Es eignet sich hervorragend für kurze Dialoge, Notizen und schnelle Inhaltserstellung."
  },
  "nousresearch/hermes-2-pro-llama-3-8b": {
    "description": "Hermes 2 Pro Llama 3 8B ist die aktualisierte Version von Nous Hermes 2 und enthält die neuesten intern entwickelten Datensätze."
  },
  "o1-mini": {
    "description": "o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das für Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "o1-preview": {
    "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "open-codestral-mamba": {
    "description": "Codestral Mamba ist ein auf die Codegenerierung spezialisiertes Mamba 2-Sprachmodell, das starke Unterstützung für fortschrittliche Code- und Schlussfolgerungsaufgaben bietet."
  },
  "open-mistral-7b": {
    "description": "Mistral 7B ist ein kompaktes, aber leistungsstarkes Modell, das sich gut für Batch-Verarbeitung und einfache Aufgaben wie Klassifizierung und Textgenerierung eignet und über gute Schlussfolgerungsfähigkeiten verfügt."
  },
  "open-mistral-nemo": {
    "description": "Mistral Nemo ist ein 12B-Modell, das in Zusammenarbeit mit Nvidia entwickelt wurde und hervorragende Schlussfolgerungs- und Codierungsfähigkeiten bietet, die leicht zu integrieren und zu ersetzen sind."
  },
  "open-mixtral-8x22b": {
    "description": "Mixtral 8x22B ist ein größeres Expertenmodell, das sich auf komplexe Aufgaben konzentriert und hervorragende Schlussfolgerungsfähigkeiten sowie eine höhere Durchsatzrate bietet."
  },
  "open-mixtral-8x7b": {
    "description": "Mixtral 8x7B ist ein spärliches Expertenmodell, das mehrere Parameter nutzt, um die Schlussfolgerungsgeschwindigkeit zu erhöhen und sich für die Verarbeitung mehrsprachiger und Codegenerierungsaufgaben eignet."
  },
  "openai/gpt-4o-2024-08-06": {
    "description": "ChatGPT-4o ist ein dynamisches Modell, das in Echtzeit aktualisiert wird, um die neueste Version zu gewährleisten. Es kombiniert starke Sprachverständnis- und Generierungsfähigkeiten und eignet sich für großangelegte Anwendungsszenarien, einschließlich Kundenservice, Bildung und technische Unterstützung."
  },
  "openai/gpt-4o-mini": {
    "description": "GPT-4o mini ist das neueste Modell von OpenAI, das nach GPT-4 Omni veröffentlicht wurde und Text- und Bild-Eingaben unterstützt. Als ihr fortschrittlichstes kleines Modell ist es viel günstiger als andere neueste Modelle und über 60 % günstiger als GPT-3.5 Turbo. Es behält die fortschrittlichste Intelligenz bei und bietet gleichzeitig ein hervorragendes Preis-Leistungs-Verhältnis. GPT-4o mini erzielte 82 % im MMLU-Test und rangiert derzeit in den Chat-Präferenzen über GPT-4."
  },
  "openai/o1-mini": {
    "description": "o1-mini ist ein schnelles und kosteneffizientes Inferenzmodell, das für Programmier-, Mathematik- und Wissenschaftsanwendungen entwickelt wurde. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "openai/o1-preview": {
    "description": "o1 ist OpenAIs neues Inferenzmodell, das für komplexe Aufgaben geeignet ist, die umfangreiches Allgemeinwissen erfordern. Das Modell hat einen Kontext von 128K und einen Wissensstand bis Oktober 2023."
  },
  "openchat/openchat-7b": {
    "description": "OpenChat 7B ist eine Open-Source-Sprachmodellbibliothek, die mit der Strategie „C-RLFT (Conditional Reinforcement Learning Fine-Tuning)“ optimiert wurde."
  },
  "openrouter/auto": {
    "description": "Je nach Kontextlänge, Thema und Komplexität wird Ihre Anfrage an Llama 3 70B Instruct, Claude 3.5 Sonnet (selbstregulierend) oder GPT-4o gesendet."
  },
  "phi3": {
    "description": "Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das für effiziente Integration und großangelegte Wissensschlüsse geeignet ist."
  },
  "phi3:14b": {
    "description": "Phi-3 ist ein leichtgewichtiges offenes Modell von Microsoft, das für effiziente Integration und großangelegte Wissensschlüsse geeignet ist."
  },
  "pixtral-12b-2409": {
    "description": "Das Pixtral-Modell zeigt starke Fähigkeiten in Aufgaben wie Diagramm- und Bildverständnis, Dokumentenfragen, multimodale Schlussfolgerungen und Befolgung von Anweisungen. Es kann Bilder in natürlicher Auflösung und Seitenverhältnis aufnehmen und in einem langen Kontextfenster von bis zu 128K Tokens beliebig viele Bilder verarbeiten."
  },
  "qwen-coder-turbo-latest": {
    "description": "Das Tongyi Qianwen Code-Modell."
  },
  "qwen-long": {
    "description": "Qwen ist ein groß angelegtes Sprachmodell, das lange Textkontexte unterstützt und Dialogfunktionen für verschiedene Szenarien wie lange Dokumente und mehrere Dokumente bietet."
  },
  "qwen-math-plus-latest": {
    "description": "Das Tongyi Qianwen Mathematikmodell ist speziell für die Lösung von mathematischen Problemen konzipiert."
  },
  "qwen-math-turbo-latest": {
    "description": "Das Tongyi Qianwen Mathematikmodell ist speziell für die Lösung von mathematischen Problemen konzipiert."
  },
  "qwen-max-latest": {
    "description": "Der Tongyi Qianwen ist ein Sprachmodell mit einem Umfang von mehreren Billionen, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt und die API-Modelle hinter der aktuellen Version 2.5 von Tongyi Qianwen darstellt."
  },
  "qwen-plus-latest": {
    "description": "Der Tongyi Qianwen ist die erweiterte Version eines groß angelegten Sprachmodells, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-turbo-latest": {
    "description": "Der Tongyi Qianwen ist ein groß angelegtes Sprachmodell, das Eingaben in verschiedenen Sprachen wie Chinesisch und Englisch unterstützt."
  },
  "qwen-vl-chat-v1": {
    "description": "Qwen VL unterstützt flexible Interaktionsmethoden, einschließlich Mehrbild-, Mehrfachfragen und kreativen Fähigkeiten."
  },
  "qwen-vl-max": {
    "description": "Qwen ist ein groß angelegtes visuelles Sprachmodell. Im Vergleich zur verbesserten Version hat es die visuelle Schlussfolgerungsfähigkeit und die Befolgung von Anweisungen weiter verbessert und bietet ein höheres Maß an visueller Wahrnehmung und Kognition."
  },
  "qwen-vl-plus": {
    "description": "Qwen ist eine verbesserte Version des groß angelegten visuellen Sprachmodells. Es verbessert erheblich die Fähigkeit zur Detailerkennung und Texterkennung und unterstützt Bilder mit über einer Million Pixeln und beliebigen Seitenverhältnissen."
  },
  "qwen-vl-v1": {
    "description": "Initiiert mit dem Qwen-7B-Sprachmodell, fügt es ein Bildmodell hinzu, das für Bildeingaben mit einer Auflösung von 448 vortrainiert wurde."
  },
  "qwen/qwen-2-7b-instruct:free": {
    "description": "Qwen2 ist eine neue Serie großer Sprachmodelle mit stärkeren Verständnis- und Generierungsfähigkeiten."
  },
  "qwen2": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2.5-14b-instruct": {
    "description": "Das 14B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-32b-instruct": {
    "description": "Das 32B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-72b-instruct": {
    "description": "Das 72B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-7b-instruct": {
    "description": "Das 7B-Modell von Tongyi Qianwen 2.5 ist öffentlich zugänglich."
  },
  "qwen2.5-coder-1.5b-instruct": {
    "description": "Die Open-Source-Version des Tongyi Qianwen Code-Modells."
  },
  "qwen2.5-coder-7b-instruct": {
    "description": "Die Open-Source-Version des Tongyi Qianwen Code-Modells."
  },
  "qwen2.5-math-1.5b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2.5-math-72b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2.5-math-7b-instruct": {
    "description": "Das Qwen-Math-Modell verfügt über starke Fähigkeiten zur Lösung mathematischer Probleme."
  },
  "qwen2:0.5b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2:1.5b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "qwen2:72b": {
    "description": "Qwen2 ist das neue große Sprachmodell von Alibaba, das mit hervorragender Leistung eine Vielzahl von Anwendungsanforderungen unterstützt."
  },
  "solar-1-mini-chat": {
    "description": "Solar Mini ist ein kompaktes LLM, das besser als GPT-3.5 abschneidet und über starke Mehrsprachigkeitsfähigkeiten verfügt, unterstützt Englisch und Koreanisch und bietet eine effiziente, kompakte Lösung."
  },
  "solar-1-mini-chat-ja": {
    "description": "Solar Mini (Ja) erweitert die Fähigkeiten von Solar Mini und konzentriert sich auf Japanisch, während es gleichzeitig in der Nutzung von Englisch und Koreanisch effizient und leistungsstark bleibt."
  },
  "solar-pro": {
    "description": "Solar Pro ist ein hochintelligentes LLM, das von Upstage entwickelt wurde und sich auf die Befolgung von Anweisungen mit einer einzigen GPU konzentriert, mit einem IFEval-Score von über 80. Derzeit unterstützt es Englisch, die offizielle Version ist für November 2024 geplant und wird die Sprachunterstützung und Kontextlänge erweitern."
  },
  "step-1-128k": {
    "description": "Bietet ein ausgewogenes Verhältnis zwischen Leistung und Kosten, geeignet für allgemeine Szenarien."
  },
  "step-1-256k": {
    "description": "Verfügt über die Fähigkeit zur Verarbeitung ultra-langer Kontexte, besonders geeignet für die Analyse langer Dokumente."
  },
  "step-1-32k": {
    "description": "Unterstützt mittellange Dialoge und eignet sich für verschiedene Anwendungsszenarien."
  },
  "step-1-8k": {
    "description": "Kleinmodell, geeignet für leichte Aufgaben."
  },
  "step-1-flash": {
    "description": "Hochgeschwindigkeitsmodell, geeignet für Echtzeitdialoge."
  },
  "step-1v-32k": {
    "description": "Unterstützt visuelle Eingaben und verbessert die multimodale Interaktionserfahrung."
  },
  "step-1v-8k": {
    "description": "Kleinvisualmodell, geeignet für grundlegende Text- und Bildaufgaben."
  },
  "step-2-16k": {
    "description": "Unterstützt groß angelegte Kontextinteraktionen und eignet sich für komplexe Dialogszenarien."
  },
  "taichu_llm": {
    "description": "Das Zīdōng Taichu Sprachmodell verfügt über außergewöhnliche Sprachverständnisfähigkeiten sowie Fähigkeiten in Textgenerierung, Wissensabfrage, Programmierung, mathematischen Berechnungen, logischem Denken, Sentimentanalyse und Textzusammenfassung. Es kombiniert innovativ große Datenvortrainings mit reichhaltigem Wissen aus mehreren Quellen, verfeinert kontinuierlich die Algorithmen und absorbiert ständig neues Wissen aus umfangreichen Textdaten in Bezug auf Vokabular, Struktur, Grammatik und Semantik, um die Leistung des Modells kontinuierlich zu verbessern. Es bietet den Nutzern bequemere Informationen und Dienstleistungen sowie ein intelligenteres Erlebnis."
  },
  "togethercomputer/StripedHyena-Nous-7B": {
    "description": "StripedHyena Nous (7B) bietet durch effiziente Strategien und Modellarchitekturen verbesserte Rechenfähigkeiten."
  },
  "upstage/SOLAR-10.7B-Instruct-v1.0": {
    "description": "Upstage SOLAR Instruct v1 (11B) eignet sich für präzise Anweisungsaufgaben und bietet hervorragende Sprachverarbeitungsfähigkeiten."
  },
  "wizardlm2": {
    "description": "WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."
  },
  "wizardlm2:8x22b": {
    "description": "WizardLM 2 ist ein Sprachmodell von Microsoft AI, das in komplexen Dialogen, mehrsprachigen Anwendungen, Schlussfolgerungen und intelligenten Assistenten besonders gut abschneidet."
  },
  "yi-large": {
    "description": "Das brandneue Modell mit einer Billion Parametern bietet außergewöhnliche Frage- und Textgenerierungsfähigkeiten."
  },
  "yi-large-fc": {
    "description": "Basierend auf dem yi-large-Modell unterstützt und verstärkt es die Fähigkeit zu Werkzeugaufrufen und eignet sich für verschiedene Geschäftsszenarien, die den Aufbau von Agenten oder Workflows erfordern."
  },
  "yi-large-preview": {
    "description": "Frühe Version, empfohlen wird die Verwendung von yi-large (neue Version)."
  },
  "yi-large-rag": {
    "description": "Ein fortgeschrittener Dienst, der auf dem leistungsstarken yi-large-Modell basiert und präzise Antworten durch die Kombination von Abruf- und Generierungstechnologien bietet, sowie Echtzeit-Informationsdienste aus dem gesamten Web."
  },
  "yi-large-turbo": {
    "description": "Hervorragendes Preis-Leistungs-Verhältnis und außergewöhnliche Leistung. Hochpräzise Feinabstimmung basierend auf Leistung, Schlussfolgerungsgeschwindigkeit und Kosten."
  },
  "yi-medium": {
    "description": "Mittelgroßes Modell mit verbesserten Feinabstimmungen, ausgewogene Fähigkeiten und gutes Preis-Leistungs-Verhältnis. Tiefgehende Optimierung der Anweisungsbefolgung."
  },
  "yi-medium-200k": {
    "description": "200K ultra-lange Kontextfenster bieten tiefes Verständnis und Generierungsfähigkeiten für lange Texte."
  },
  "yi-spark": {
    "description": "Klein und kompakt, ein leichtgewichtiges und schnelles Modell. Bietet verbesserte mathematische Berechnungs- und Programmierfähigkeiten."
  },
  "yi-vision": {
    "description": "Modell für komplexe visuelle Aufgaben, das hohe Leistungsfähigkeit bei der Bildverarbeitung und -analyse bietet."
  }
}
